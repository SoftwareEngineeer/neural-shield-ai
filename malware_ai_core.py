# malware_ai_core.py

import random
import joblib
import hashlib
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

MODEL_PATH = "ai_firewall.pkl"

# --- 1. Mutate File for Adversarial Attack ---
def mutate_file(content):
    content = bytearray(content)
    for _ in range(random.randint(5, 15)):
        index = random.randint(0, len(content) - 1)
        content[index] ^= random.randint(1, 255)
    return bytes(content)

# --- 2. Feature Extractor ---
def extract_features(content):
    size = len(content)
    entropy = sum([1 for b in content if b > 128]) / size
    mutation = 1  # since it's adversarial
    return [size, entropy, mutation]

# --- 3. Test Adversarial File Against AI ---
def test_against_ai(content):
    try:
        model = joblib.load(MODEL_PATH)
        features = extract_features(content)
        prediction = model.predict([features])[0]
        return prediction == 0  # True = attack successful
    except Exception as e:
        print("Model error:", e)
        return False

# --- 4. Generate Fake Training Data ---
def simulate_training_data(n=200):
    data = []
    for _ in range(n):
        size = random.randint(1024, 50000)
        entropy = random.uniform(0.1, 8.0)
        mutation = random.choice([0, 1])
        label = 1 if entropy > 5 or mutation == 1 else 0
        data.append([size, entropy, mutation, label])
    return pd.DataFrame(data, columns=["Size", "Entropy", "Mutation", "Label"])

# --- 5. Train AI Firewall ---
def train_firewall_model():
    df = simulate_training_data()
    X = df[["Size", "Entropy", "Mutation"]]
    y = df["Label"]

    model = RandomForestClassifier(n_estimators=100)
    model.fit(X, y)

    joblib.dump(model, MODEL_PATH)

    df["Detected"] = model.predict(X)
    acc = accuracy_score(y, df["Detected"]) * 100
    return acc, df
